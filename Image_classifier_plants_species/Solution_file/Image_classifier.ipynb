{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Image_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnnqX8_EOMAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4DfItJYOMAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def warn(*args, **kwargs): pass\n",
        "import warnings\n",
        "warnings.warn = warn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pAHBzeLOMAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedShuffleSplit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WeF2Z4aOMAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('https://s3.amazonaws.com/hackerday.datascience/118/train.csv')\n",
        "test = pd.read_csv('https://s3.amazonaws.com/hackerday.datascience/118/test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iqeVYuiOMAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Preparation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYwdRPpWOMAU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "a506a4d5-7322-41ea-c062-243daeb16648"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>species</th>\n",
              "      <th>margin1</th>\n",
              "      <th>margin2</th>\n",
              "      <th>margin3</th>\n",
              "      <th>margin4</th>\n",
              "      <th>margin5</th>\n",
              "      <th>margin6</th>\n",
              "      <th>margin7</th>\n",
              "      <th>margin8</th>\n",
              "      <th>margin9</th>\n",
              "      <th>margin10</th>\n",
              "      <th>margin11</th>\n",
              "      <th>margin12</th>\n",
              "      <th>margin13</th>\n",
              "      <th>margin14</th>\n",
              "      <th>margin15</th>\n",
              "      <th>margin16</th>\n",
              "      <th>margin17</th>\n",
              "      <th>margin18</th>\n",
              "      <th>margin19</th>\n",
              "      <th>margin20</th>\n",
              "      <th>margin21</th>\n",
              "      <th>margin22</th>\n",
              "      <th>margin23</th>\n",
              "      <th>margin24</th>\n",
              "      <th>margin25</th>\n",
              "      <th>margin26</th>\n",
              "      <th>margin27</th>\n",
              "      <th>margin28</th>\n",
              "      <th>margin29</th>\n",
              "      <th>margin30</th>\n",
              "      <th>margin31</th>\n",
              "      <th>margin32</th>\n",
              "      <th>margin33</th>\n",
              "      <th>margin34</th>\n",
              "      <th>margin35</th>\n",
              "      <th>margin36</th>\n",
              "      <th>margin37</th>\n",
              "      <th>margin38</th>\n",
              "      <th>...</th>\n",
              "      <th>texture25</th>\n",
              "      <th>texture26</th>\n",
              "      <th>texture27</th>\n",
              "      <th>texture28</th>\n",
              "      <th>texture29</th>\n",
              "      <th>texture30</th>\n",
              "      <th>texture31</th>\n",
              "      <th>texture32</th>\n",
              "      <th>texture33</th>\n",
              "      <th>texture34</th>\n",
              "      <th>texture35</th>\n",
              "      <th>texture36</th>\n",
              "      <th>texture37</th>\n",
              "      <th>texture38</th>\n",
              "      <th>texture39</th>\n",
              "      <th>texture40</th>\n",
              "      <th>texture41</th>\n",
              "      <th>texture42</th>\n",
              "      <th>texture43</th>\n",
              "      <th>texture44</th>\n",
              "      <th>texture45</th>\n",
              "      <th>texture46</th>\n",
              "      <th>texture47</th>\n",
              "      <th>texture48</th>\n",
              "      <th>texture49</th>\n",
              "      <th>texture50</th>\n",
              "      <th>texture51</th>\n",
              "      <th>texture52</th>\n",
              "      <th>texture53</th>\n",
              "      <th>texture54</th>\n",
              "      <th>texture55</th>\n",
              "      <th>texture56</th>\n",
              "      <th>texture57</th>\n",
              "      <th>texture58</th>\n",
              "      <th>texture59</th>\n",
              "      <th>texture60</th>\n",
              "      <th>texture61</th>\n",
              "      <th>texture62</th>\n",
              "      <th>texture63</th>\n",
              "      <th>texture64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Acer_Opalus</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.066406</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.012695</td>\n",
              "      <td>0.028320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.026367</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>0.034180</td>\n",
              "      <td>0.056641</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.022461</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Pterocarya_Stenoptera</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.048828</td>\n",
              "      <td>0.054688</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050781</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.104490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.061523</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.053711</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.022461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Quercus_Hartwissiana</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.068359</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.022461</td>\n",
              "      <td>0.020508</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.056641</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020508</td>\n",
              "      <td>0.002930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>Tilia_Tomentosa</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.124020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.096680</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>0.126950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>0.055664</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.022461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020508</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>Quercus_Variabilis</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.048828</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.083984</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040039</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110350</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.087891</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.061523</td>\n",
              "      <td>0.026367</td>\n",
              "      <td>0.132810</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.096680</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 194 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                species   margin1  ...  texture62  texture63  texture64\n",
              "0   1            Acer_Opalus  0.007812  ...   0.004883   0.000000   0.025391\n",
              "1   2  Pterocarya_Stenoptera  0.005859  ...   0.000977   0.039062   0.022461\n",
              "2   3   Quercus_Hartwissiana  0.005859  ...   0.000000   0.020508   0.002930\n",
              "3   5        Tilia_Tomentosa  0.000000  ...   0.017578   0.000000   0.047852\n",
              "4   6     Quercus_Variabilis  0.005859  ...   0.000000   0.000000   0.031250\n",
              "\n",
              "[5 rows x 194 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ7KXMnAOMAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "f9323af1-0845-4d47-b758-c546a382da81"
      },
      "source": [
        "train.species.value_counts()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Morus_Nigra               10\n",
              "Quercus_Chrysolepis       10\n",
              "Populus_Grandidentata     10\n",
              "Acer_Capillipes           10\n",
              "Magnolia_Heptapeta        10\n",
              "                          ..\n",
              "Quercus_Kewensis          10\n",
              "Tilia_Platyphyllos        10\n",
              "Quercus_Shumardii         10\n",
              "Quercus_Coccifera         10\n",
              "Quercus_Semecarpifolia    10\n",
              "Name: species, Length: 99, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98FSEskqOMAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder().fit(train.species) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9v1CCrdOMAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = le.transform(train.species) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fxfS9H_OMAd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e7b9742-2ed1-4bbb-8649-2c6f30dea1d0"
      },
      "source": [
        "labels"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, 49, 65, 94, 84, 40, 54, 78, 53, 89, 98, 16, 74, 50, 58, 31, 43,\n",
              "        4, 75, 44, 83, 84, 13, 66, 15,  6, 73, 22, 73, 31, 36, 27, 94, 88,\n",
              "       12, 28, 21, 25, 20, 60, 84, 65, 69, 58, 23, 76, 18, 52, 54,  9, 48,\n",
              "       47, 64, 81, 83, 36, 58, 21, 81, 20, 62, 88, 34, 92, 79, 82, 20, 32,\n",
              "        4, 84, 36, 35, 72, 60, 71, 72, 52, 50, 54, 11, 51, 18, 47,  5,  8,\n",
              "       37, 97, 20, 33,  1, 59,  1, 56,  1,  9, 57, 20, 79, 29, 16, 32, 54,\n",
              "       93, 10, 46, 59, 84, 76, 15, 10, 15,  0, 69,  4, 51, 51, 94, 36, 39,\n",
              "       62,  2, 24, 26, 35, 25, 87,  0, 55, 34, 38,  1, 45,  7, 93, 56, 38,\n",
              "       21, 51, 75, 81, 74, 33, 20, 37,  9, 40, 60, 31, 83, 50, 71, 67, 30,\n",
              "       66,  1, 43, 61, 23, 65, 84, 87, 46, 57, 16,  2, 28, 12, 96, 44, 76,\n",
              "       29, 75, 41, 87, 67, 61, 30,  5, 12, 62,  3, 83, 81,  6, 85,  4, 37,\n",
              "       57, 84, 39, 71, 61,  6, 76, 14, 31, 98, 40, 17, 51, 16, 42, 63, 86,\n",
              "       37, 69, 86, 71, 80, 78, 14, 35, 25,  5, 39,  8,  9, 26, 44, 60, 13,\n",
              "       14, 77, 13, 80, 87, 18, 60, 78, 92, 51, 45, 78, 41, 51, 30, 14, 35,\n",
              "       46, 21,  8,  6, 92, 38, 40, 15, 32, 17, 93, 71, 92, 27, 78, 15, 19,\n",
              "       60, 21, 38, 36, 49, 74, 67, 95, 31, 82, 45, 16, 83, 63, 80, 42, 22,\n",
              "       74, 53, 15, 44, 47, 57, 94, 76, 17, 32, 24, 15, 93, 24, 80, 59, 46,\n",
              "       12, 51, 77, 79, 70, 69, 16,  2, 63, 83, 55, 12, 53,  1, 67,  0,  2,\n",
              "       36, 42, 10,  9, 52, 59,  6, 22, 86, 31, 51, 37, 43, 75, 90, 24, 86,\n",
              "       96, 45, 32, 98, 36, 66, 48, 73, 73, 79, 56, 41, 21, 25, 27, 97, 18,\n",
              "       44, 45, 40, 80, 63, 20, 35,  0,  8, 27, 25, 35, 59, 61, 21, 37, 29,\n",
              "        6, 19, 78, 50, 54, 37, 93, 33, 46, 79, 59, 29, 43,  0, 23, 17, 38,\n",
              "       66, 38, 89, 17, 25, 31, 65, 10, 26, 86, 58, 42, 46, 24, 95, 93,  8,\n",
              "       53, 32, 14, 10, 94,  8,  8, 64, 44, 74, 30, 97, 22, 11, 68, 56, 90,\n",
              "       96, 16, 43, 57, 91, 24, 28, 82, 90, 64, 61, 92, 28, 84, 70, 45, 85,\n",
              "       34,  7, 88, 89, 61, 26, 88, 41, 46,  8, 91, 41, 14, 98, 28, 26, 36,\n",
              "       70, 74,  7, 52, 70, 42, 66, 22, 13, 44, 91, 53, 22, 16, 40, 40, 28,\n",
              "       70,  6, 60, 95, 23, 16, 50, 29, 49,  9, 18, 55, 63, 60, 19, 28, 30,\n",
              "       31, 85, 66, 88, 63, 83, 64, 96, 13, 34, 27, 95, 36, 72, 29, 91, 22,\n",
              "       65, 71, 66, 11, 32,  2, 75, 39,  5, 37, 67, 81, 55, 61, 57, 81, 82,\n",
              "       63, 55, 54, 35, 86, 25, 24, 96, 10, 58, 59, 28, 89, 54, 52, 85, 68,\n",
              "       69,  8, 39, 95, 39, 82, 48, 74, 52, 74, 55,  9, 47, 84, 91, 12, 96,\n",
              "       82, 64,  7, 40, 73, 77, 11, 36, 68, 23, 28, 46, 75, 43,  2, 11, 47,\n",
              "       53, 56, 62, 62, 80, 56, 30,  3, 88, 37, 33, 73, 76, 21,  5, 76, 87,\n",
              "       68, 83, 62, 57, 47, 19, 88, 96, 42, 23, 44, 87, 82, 49, 63, 24, 94,\n",
              "       69, 54,  5, 79, 43, 12, 50,  5, 52, 92,  4, 84,  1, 33, 49, 26, 18,\n",
              "       44, 13, 24, 73, 89, 78, 67, 41, 11, 46, 47, 69,  0, 18, 98, 44, 85,\n",
              "       29, 53,  1, 45,  3,  9, 13,  2, 66, 59, 79,  6, 17, 43, 83, 26,  1,\n",
              "       12, 49, 71, 89, 58, 93, 39, 42, 15, 38, 55, 15, 93,  4, 90, 88, 55,\n",
              "       40, 55, 17, 34, 94, 57, 92, 81, 26, 60, 89, 49, 89, 30, 65, 58,  4,\n",
              "       19,  4, 76, 74, 71, 21, 54, 13, 16, 72, 68, 62, 61, 25, 72,  7, 12,\n",
              "       18, 77, 90, 62, 14,  3, 78, 65, 37, 27, 50, 95, 98, 60, 72, 58, 38,\n",
              "       87, 93, 19,  7, 83, 50,  3, 91, 77,  7, 64, 61, 69, 23, 76, 65, 48,\n",
              "       41, 92, 20, 91, 18, 70,  9,  9, 29, 85, 67,  0, 35, 98, 91, 90, 31,\n",
              "       53, 39, 24, 85, 96, 17,  7, 11, 96, 39, 56, 90, 79, 45, 64, 97, 41,\n",
              "       19, 74, 11, 10, 62, 95, 28, 96, 10,  7, 68,  7, 93, 34, 42, 68, 41,\n",
              "       14, 22, 58, 12, 71, 27, 98, 72, 91,  3, 43, 19, 61, 75, 20, 81, 63,\n",
              "       67, 56, 26, 47, 11, 31, 57, 62, 66, 19, 75, 97, 94, 13, 75, 95, 32,\n",
              "       50, 97, 52, 87, 32,  3, 47, 77, 48, 33, 73, 64, 49, 68, 43, 94, 77,\n",
              "       68, 47, 82,  2, 30, 23, 33, 34, 66, 33, 35, 88, 68, 27, 87, 54, 79,\n",
              "       34, 67, 65, 18,  4, 26, 30, 52, 86,  0, 29, 80, 67, 95, 39, 25, 70,\n",
              "       58, 35, 27, 17, 38, 91, 13, 23, 77, 79, 77, 22, 49, 98, 48, 46, 48,\n",
              "        5, 63, 97, 80, 53, 20, 25, 78, 10, 65, 33, 41, 85, 90, 98, 97, 71,\n",
              "       95, 52,  3, 29, 69, 51, 70, 27, 22, 34,  6, 48, 72, 21, 89, 17, 97,\n",
              "       72, 80, 10, 57, 64, 92, 38, 15, 73, 87, 73, 48, 42, 82, 33, 56,  3,\n",
              "       42,  1, 53, 55, 90, 19,  6, 30, 86, 64, 49,  2,  8, 45, 76, 92,  0,\n",
              "       23, 69, 59, 80, 90, 32,  5, 59, 85, 89, 94, 45, 48, 86, 81, 14,  4,\n",
              "       77, 56, 82,  2, 85, 70, 88,  0, 75, 14, 86, 81, 97, 70, 72, 34, 40,\n",
              "        5, 11, 78, 50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klcn3VaJOMAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "7808324d-056b-4c92-92c9-b4bc1e265279"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>margin1</th>\n",
              "      <th>margin2</th>\n",
              "      <th>margin3</th>\n",
              "      <th>margin4</th>\n",
              "      <th>margin5</th>\n",
              "      <th>margin6</th>\n",
              "      <th>margin7</th>\n",
              "      <th>margin8</th>\n",
              "      <th>margin9</th>\n",
              "      <th>margin10</th>\n",
              "      <th>margin11</th>\n",
              "      <th>margin12</th>\n",
              "      <th>margin13</th>\n",
              "      <th>margin14</th>\n",
              "      <th>margin15</th>\n",
              "      <th>margin16</th>\n",
              "      <th>margin17</th>\n",
              "      <th>margin18</th>\n",
              "      <th>margin19</th>\n",
              "      <th>margin20</th>\n",
              "      <th>margin21</th>\n",
              "      <th>margin22</th>\n",
              "      <th>margin23</th>\n",
              "      <th>margin24</th>\n",
              "      <th>margin25</th>\n",
              "      <th>margin26</th>\n",
              "      <th>margin27</th>\n",
              "      <th>margin28</th>\n",
              "      <th>margin29</th>\n",
              "      <th>margin30</th>\n",
              "      <th>margin31</th>\n",
              "      <th>margin32</th>\n",
              "      <th>margin33</th>\n",
              "      <th>margin34</th>\n",
              "      <th>margin35</th>\n",
              "      <th>margin36</th>\n",
              "      <th>margin37</th>\n",
              "      <th>margin38</th>\n",
              "      <th>margin39</th>\n",
              "      <th>...</th>\n",
              "      <th>texture25</th>\n",
              "      <th>texture26</th>\n",
              "      <th>texture27</th>\n",
              "      <th>texture28</th>\n",
              "      <th>texture29</th>\n",
              "      <th>texture30</th>\n",
              "      <th>texture31</th>\n",
              "      <th>texture32</th>\n",
              "      <th>texture33</th>\n",
              "      <th>texture34</th>\n",
              "      <th>texture35</th>\n",
              "      <th>texture36</th>\n",
              "      <th>texture37</th>\n",
              "      <th>texture38</th>\n",
              "      <th>texture39</th>\n",
              "      <th>texture40</th>\n",
              "      <th>texture41</th>\n",
              "      <th>texture42</th>\n",
              "      <th>texture43</th>\n",
              "      <th>texture44</th>\n",
              "      <th>texture45</th>\n",
              "      <th>texture46</th>\n",
              "      <th>texture47</th>\n",
              "      <th>texture48</th>\n",
              "      <th>texture49</th>\n",
              "      <th>texture50</th>\n",
              "      <th>texture51</th>\n",
              "      <th>texture52</th>\n",
              "      <th>texture53</th>\n",
              "      <th>texture54</th>\n",
              "      <th>texture55</th>\n",
              "      <th>texture56</th>\n",
              "      <th>texture57</th>\n",
              "      <th>texture58</th>\n",
              "      <th>texture59</th>\n",
              "      <th>texture60</th>\n",
              "      <th>texture61</th>\n",
              "      <th>texture62</th>\n",
              "      <th>texture63</th>\n",
              "      <th>texture64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.078125</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.076172</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>0.064453</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.045898</td>\n",
              "      <td>0.024414</td>\n",
              "      <td>0.045898</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.070312</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054688</td>\n",
              "      <td>0.024414</td>\n",
              "      <td>0.026367</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.020508</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.034180</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014648</td>\n",
              "      <td>0.018555</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.053711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.064453</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.051758</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.058594</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.012695</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.047852</td>\n",
              "      <td>0.030273</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.041992</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.049805</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.044922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.046875</td>\n",
              "      <td>0.056641</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.088867</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.051758</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.026367</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.022461</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.038086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.084961</td>\n",
              "      <td>0.128910</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.050781</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.063477</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.022461</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.026367</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040039</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.026367</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.018555</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.077148</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.040039</td>\n",
              "      <td>0.012695</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.089844</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.056641</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.074219</td>\n",
              "      <td>0.083984</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014648</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.034180</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.052734</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.028320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.054688</td>\n",
              "      <td>0.073242</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.014648</td>\n",
              "      <td>0.040039</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.007812</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 193 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id   margin1   margin2   margin3  ...  texture61  texture62  texture63  texture64\n",
              "0   4  0.019531  0.009766  0.078125  ...        0.0   0.000000   0.003906   0.053711\n",
              "1   7  0.007812  0.005859  0.064453  ...        0.0   0.000977   0.037109   0.044922\n",
              "2   9  0.000000  0.000000  0.001953  ...        0.0   0.015625   0.000000   0.000000\n",
              "3  12  0.000000  0.000000  0.009766  ...        0.0   0.089844   0.000000   0.008789\n",
              "4  13  0.001953  0.000000  0.015625  ...        0.0   0.007812   0.009766   0.007812\n",
              "\n",
              "[5 rows x 193 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEhO2PfeOMAg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "58e51c9e-7491-4686-ee79-19752d764338"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 990 entries, 0 to 989\n",
            "Columns: 194 entries, id to texture64\n",
            "dtypes: float64(192), int64(1), object(1)\n",
            "memory usage: 1.5+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-z3kORuOMAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8fc7d911-82ca-4e97-a757-f92e6a071aa2"
      },
      "source": [
        "test.info()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 594 entries, 0 to 593\n",
            "Columns: 193 entries, id to texture64\n",
            "dtypes: float64(192), int64(1)\n",
            "memory usage: 895.8 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB9qUrJZOMAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a function to organize both training and test dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-GJfIcQOMAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(train, test):\n",
        "    le = LabelEncoder().fit(train.species) \n",
        "    labels = le.transform(train.species)           # encode species strings\n",
        "    classes = list(le.classes_)                    \n",
        "    test_ids = test.id                             \n",
        "    \n",
        "    train = train.drop(['species', 'id'], axis=1)  \n",
        "    test = test.drop(['id'], axis=1)\n",
        "    \n",
        "    return train, labels, test, test_ids, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbNifL3vOMAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "daa694b9-2d9d-4969-d19a-06b8c3fa9c6a"
      },
      "source": [
        "train, labels, test, test_ids, classes = encode(train, test)\n",
        "train.head()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>margin1</th>\n",
              "      <th>margin2</th>\n",
              "      <th>margin3</th>\n",
              "      <th>margin4</th>\n",
              "      <th>margin5</th>\n",
              "      <th>margin6</th>\n",
              "      <th>margin7</th>\n",
              "      <th>margin8</th>\n",
              "      <th>margin9</th>\n",
              "      <th>margin10</th>\n",
              "      <th>margin11</th>\n",
              "      <th>margin12</th>\n",
              "      <th>margin13</th>\n",
              "      <th>margin14</th>\n",
              "      <th>margin15</th>\n",
              "      <th>margin16</th>\n",
              "      <th>margin17</th>\n",
              "      <th>margin18</th>\n",
              "      <th>margin19</th>\n",
              "      <th>margin20</th>\n",
              "      <th>margin21</th>\n",
              "      <th>margin22</th>\n",
              "      <th>margin23</th>\n",
              "      <th>margin24</th>\n",
              "      <th>margin25</th>\n",
              "      <th>margin26</th>\n",
              "      <th>margin27</th>\n",
              "      <th>margin28</th>\n",
              "      <th>margin29</th>\n",
              "      <th>margin30</th>\n",
              "      <th>margin31</th>\n",
              "      <th>margin32</th>\n",
              "      <th>margin33</th>\n",
              "      <th>margin34</th>\n",
              "      <th>margin35</th>\n",
              "      <th>margin36</th>\n",
              "      <th>margin37</th>\n",
              "      <th>margin38</th>\n",
              "      <th>margin39</th>\n",
              "      <th>margin40</th>\n",
              "      <th>...</th>\n",
              "      <th>texture25</th>\n",
              "      <th>texture26</th>\n",
              "      <th>texture27</th>\n",
              "      <th>texture28</th>\n",
              "      <th>texture29</th>\n",
              "      <th>texture30</th>\n",
              "      <th>texture31</th>\n",
              "      <th>texture32</th>\n",
              "      <th>texture33</th>\n",
              "      <th>texture34</th>\n",
              "      <th>texture35</th>\n",
              "      <th>texture36</th>\n",
              "      <th>texture37</th>\n",
              "      <th>texture38</th>\n",
              "      <th>texture39</th>\n",
              "      <th>texture40</th>\n",
              "      <th>texture41</th>\n",
              "      <th>texture42</th>\n",
              "      <th>texture43</th>\n",
              "      <th>texture44</th>\n",
              "      <th>texture45</th>\n",
              "      <th>texture46</th>\n",
              "      <th>texture47</th>\n",
              "      <th>texture48</th>\n",
              "      <th>texture49</th>\n",
              "      <th>texture50</th>\n",
              "      <th>texture51</th>\n",
              "      <th>texture52</th>\n",
              "      <th>texture53</th>\n",
              "      <th>texture54</th>\n",
              "      <th>texture55</th>\n",
              "      <th>texture56</th>\n",
              "      <th>texture57</th>\n",
              "      <th>texture58</th>\n",
              "      <th>texture59</th>\n",
              "      <th>texture60</th>\n",
              "      <th>texture61</th>\n",
              "      <th>texture62</th>\n",
              "      <th>texture63</th>\n",
              "      <th>texture64</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.066406</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.029297</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.012695</td>\n",
              "      <td>0.028320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.026367</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>0.034180</td>\n",
              "      <td>0.056641</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.022461</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.037109</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.048828</td>\n",
              "      <td>0.054688</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.033203</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.050781</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.104490</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.061523</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.053711</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.022461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.068359</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.025391</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.042969</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.022461</td>\n",
              "      <td>0.020508</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.056641</td>\n",
              "      <td>0.010742</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020508</td>\n",
              "      <td>0.002930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.035156</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.039062</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>0.056641</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.019531</td>\n",
              "      <td>0.124020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.096680</td>\n",
              "      <td>0.016602</td>\n",
              "      <td>0.126950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036133</td>\n",
              "      <td>0.055664</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006836</td>\n",
              "      <td>0.022461</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.020508</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.048828</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.044922</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.011719</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015625</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.041016</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.013672</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.083984</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>0.027344</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.040039</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.005859</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110350</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.087891</td>\n",
              "      <td>0.023438</td>\n",
              "      <td>0.007812</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.017578</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.061523</td>\n",
              "      <td>0.026367</td>\n",
              "      <td>0.132810</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001953</td>\n",
              "      <td>0.096680</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021484</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.031250</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 192 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    margin1   margin2   margin3  ...  texture62  texture63  texture64\n",
              "0  0.007812  0.023438  0.023438  ...   0.004883   0.000000   0.025391\n",
              "1  0.005859  0.000000  0.031250  ...   0.000977   0.039062   0.022461\n",
              "2  0.005859  0.009766  0.019531  ...   0.000000   0.020508   0.002930\n",
              "3  0.000000  0.003906  0.023438  ...   0.017578   0.000000   0.047852\n",
              "4  0.005859  0.003906  0.048828  ...   0.000000   0.000000   0.031250\n",
              "\n",
              "[5 rows x 192 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcgOwP2iOMAr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# stratified sampling rather than simple random sampling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqX2bLKUOMAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Stratification is necessary for this dataset because there is a relatively large number of classes \n",
        "#(99 classes for 990 samples). This will ensure we have all classes represented in both the train and test indices."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8JF2IdpZM_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=23)\n",
        "sss.get_n_splits(train.values, labels)\n",
        "\n",
        "for train_index, test_index in sss.split(train.values, labels):\n",
        "    X_train, X_test = train.values[train_index], train.values[test_index]\n",
        "    y_train, y_test = labels[train_index], labels[test_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-ivvOdqOMAx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "dee489b2-b9a5-42d3-fe05-87a29d5af676"
      },
      "source": [
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(792, 192) (792,)\n",
            "(198, 192) (198,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwnYYN4iOMAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#selection of classifiers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkyhXYJfOMA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWmHhEq9OMA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize all the classifiers\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
        "    NuSVC(probability=True),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "             min_samples_leaf=1, min_samples_split=2,\n",
        "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
        "             oob_score=False, random_state=None, verbose=0,\n",
        "             warm_start=False),\n",
        "    AdaBoostClassifier(),\n",
        "    GradientBoostingClassifier(),\n",
        "    GaussianNB(),\n",
        "    LinearDiscriminantAnalysis(),\n",
        "    QuadraticDiscriminantAnalysis()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbY5_QKSOMA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize all the classifiers with the best parameters from grid search\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "            metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
        "            weights='uniform'),\n",
        "    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
        "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
        "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
        "  tol=0.001, verbose=False),\n",
        "    NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
        "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
        "   max_iter=-1, nu=0.05, probability=True, random_state=None,\n",
        "   shrinking=True, tol=0.001, verbose=False),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "             min_samples_leaf=1, min_samples_split=2,\n",
        "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
        "             oob_score=False, random_state=None, verbose=0,\n",
        "             warm_start=False),\n",
        "    AdaBoostClassifier(),\n",
        "    GradientBoostingClassifier(),\n",
        "    GaussianNB(),\n",
        "    LinearDiscriminantAnalysis(),\n",
        "    QuadraticDiscriminantAnalysis()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlGY0MFNOMA6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "c5cb384c-b0e3-4829-b87d-d646f81934e8"
      },
      "source": [
        "classifiers"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                      metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
              "                      weights='uniform'),\n",
              " SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "     decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
              "     max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
              "     verbose=False),\n",
              " NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
              "       decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
              "       max_iter=-1, nu=0.05, probability=True, random_state=None, shrinking=True,\n",
              "       tol=0.001, verbose=False),\n",
              " DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                        max_features=None, max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, presort=False,\n",
              "                        random_state=None, splitter='best'),\n",
              " RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                        min_samples_leaf=1, min_samples_split=2,\n",
              "                        min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
              "                        oob_score=False, random_state=None, verbose=0,\n",
              "                        warm_start=False),\n",
              " AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
              "                    n_estimators=50, random_state=None),\n",
              " GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
              "                            learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                            max_features=None, max_leaf_nodes=None,\n",
              "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                            min_samples_leaf=1, min_samples_split=2,\n",
              "                            min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                            n_iter_no_change=None, presort='auto',\n",
              "                            random_state=None, subsample=1.0, tol=0.0001,\n",
              "                            validation_fraction=0.1, verbose=0,\n",
              "                            warm_start=False),\n",
              " GaussianNB(priors=None, var_smoothing=1e-09),\n",
              " LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
              "                            solver='svd', store_covariance=False, tol=0.0001),\n",
              " QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,\n",
              "                               store_covariance=False, tol=0.0001)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktAujm-_OMA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logging for Visual Comparison\n",
        "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
        "log = pd.DataFrame(columns=log_cols)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVmJfaGsOMA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "073160d5-8add-4441-ac34-7d19408739dc"
      },
      "source": [
        "for clf in classifiers:\n",
        "    clf.fit(X_train, y_train)\n",
        "    name = clf.__class__.__name__\n",
        "    \n",
        "    print(\"=\"*30)\n",
        "    print(name)\n",
        "    \n",
        "    print('****Results****')\n",
        "    train_predictions = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, train_predictions)\n",
        "    print(\"Accuracy: {:.4%}\".format(acc))\n",
        "    \n",
        "    train_predictions = clf.predict_proba(X_test)\n",
        "    ll = log_loss(y_test, train_predictions)\n",
        "    print(\"Log Loss: {}\".format(ll))\n",
        "    \n",
        "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
        "    log = log.append(log_entry)\n",
        "    \n",
        "print(\"=\"*30)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============================\n",
            "KNeighborsClassifier\n",
            "****Results****\n",
            "Accuracy: 88.8889%\n",
            "Log Loss: 1.5755075129933762\n",
            "==============================\n",
            "SVC\n",
            "****Results****\n",
            "Accuracy: 81.8182%\n",
            "Log Loss: 4.461778471425003\n",
            "==============================\n",
            "NuSVC\n",
            "****Results****\n",
            "Accuracy: 96.4646%\n",
            "Log Loss: 2.296657517601855\n",
            "==============================\n",
            "DecisionTreeClassifier\n",
            "****Results****\n",
            "Accuracy: 65.6566%\n",
            "Log Loss: 11.861801994211845\n",
            "==============================\n",
            "RandomForestClassifier\n",
            "****Results****\n",
            "Accuracy: 92.4242%\n",
            "Log Loss: 1.1013302499847915\n",
            "==============================\n",
            "AdaBoostClassifier\n",
            "****Results****\n",
            "Accuracy: 4.0404%\n",
            "Log Loss: 4.198743314459643\n",
            "==============================\n",
            "GradientBoostingClassifier\n",
            "****Results****\n",
            "Accuracy: 60.1010%\n",
            "Log Loss: 2.3101444403986506\n",
            "==============================\n",
            "GaussianNB\n",
            "****Results****\n",
            "Accuracy: 57.0707%\n",
            "Log Loss: 14.827252492813216\n",
            "==============================\n",
            "LinearDiscriminantAnalysis\n",
            "****Results****\n",
            "Accuracy: 97.9798%\n",
            "Log Loss: 0.22993448213653483\n",
            "==============================\n",
            "QuadraticDiscriminantAnalysis\n",
            "****Results****\n",
            "Accuracy: 2.0202%\n",
            "Log Loss: 33.84102333642773\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgh0r31NOMA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize all the classifiers\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
        "    NuSVC(probability=True),\n",
        "    DecisionTreeClassifier(),\n",
        "    RandomForestClassifier(),\n",
        "    AdaBoostClassifier(),\n",
        "    GradientBoostingClassifier(),\n",
        "    GaussianNB(),\n",
        "    LinearDiscriminantAnalysis(),\n",
        "    QuadraticDiscriminantAnalysis()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKj6tuz-OMBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve, GridSearchCV\n",
        "def fit_model(X,y):\n",
        "    cv_sets = ShuffleSplit(n_splits=10,test_size=0.20,random_state=1234)\n",
        "    sv = SVC()\n",
        "    params = {'kernel':('linear','poly','sigmoid','rbf'),\n",
        "              'C':[0.01,0.05,0.025,0.07,0.09,1.0]\n",
        "              }\n",
        "    grid = GridSearchCV(sv,params,cv=cv_sets)\n",
        "    grid = grid.fit(X,y)\n",
        "    return grid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIIirumSOMBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "7d8e457f-031a-4631-a4e9-ba327f7bc27a"
      },
      "source": [
        "fit_model(X_train,y_train)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
              "    shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eml7Rmt9OMBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve, GridSearchCV\n",
        "\n",
        "def fit_model(X,y):\n",
        "    cv_sets = ShuffleSplit(n_splits=10,test_size=0.20,random_state=1234)\n",
        "    nsv = NuSVC()\n",
        "    params = {'kernel':('linear','poly','sigmoid','rbf'),\n",
        "              'nu':[0.01,0.05,0.025]\n",
        "              }\n",
        "    grid = GridSearchCV(nsv,params,cv=cv_sets)\n",
        "    grid = grid.fit(X,y)\n",
        "    return grid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkZGvGgkOMBH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "4c573a36-b27e-4565-ecce-ecb88d905ff0"
      },
      "source": [
        "fit_model(X_train,y_train)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
              "      decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "      kernel='linear', max_iter=-1, nu=0.05, probability=False,\n",
              "      random_state=None, shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPP6DuWROMBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve, GridSearchCV\n",
        "def fit_model(X,y):\n",
        "    cv_sets = ShuffleSplit(n_splits=10,test_size=0.20,random_state=1234)\n",
        "    dt = DecisionTreeClassifier()\n",
        "    params = {'criterion':('gini','entropy'),\n",
        "              'max_depth':[2,3,4,5]\n",
        "              }\n",
        "    grid = GridSearchCV(dt,params,cv=cv_sets)\n",
        "    grid = grid.fit(X,y)\n",
        "    return grid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X8hQuCoOMBK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "2a8a986d-59a9-46ce-d1f5-e4ee5147a23e"
      },
      "source": [
        "fit_model(X_train,y_train)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
              "                       max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort=False,\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Cj7V1QHOMBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve, GridSearchCV\n",
        "def fit_model(X,y):\n",
        "    cv_sets = ShuffleSplit(n_splits=10,test_size=0.20,random_state=1234)\n",
        "    rf = RandomForestClassifier()\n",
        "    params = {'criterion':('gini','entropy'),\n",
        "              'n_estimators':[100,200,300,500]\n",
        "              }\n",
        "    grid = GridSearchCV(rf,params,cv=cv_sets)\n",
        "    grid = grid.fit(X,y)\n",
        "    return grid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Jwk4hNnOMBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "0df92e75-a20a-4271-8a6f-ab5e6bc6ee17"
      },
      "source": [
        "fit_model(X_train,y_train)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja_wHlcDOMBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve, GridSearchCV\n",
        "def fit_model(X,y):\n",
        "    cv_sets = ShuffleSplit(n_splits=10,test_size=0.20,random_state=1234)\n",
        "    ab = AdaBoostClassifier()\n",
        "    params = {'algorithm':('SAMME', 'SAMME.R'),\n",
        "              'n_estimators':[100,200]\n",
        "              }\n",
        "    grid = GridSearchCV(ab,params,cv=cv_sets)\n",
        "    grid = grid.fit(X,y)\n",
        "    return grid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaf-ykQ0OMBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f3945503-33e4-4365-eab6-04cc370492ec"
      },
      "source": [
        "fit_model(X_train,y_train)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1.0,\n",
              "                   n_estimators=200, random_state=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNutX-UMOMBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve, GridSearchCV\n",
        "def fit_model(X,y):\n",
        "    cv_sets = ShuffleSplit(n_splits=10,test_size=0.20,random_state=1234)\n",
        "    gb = GradientBoostingClassifier()\n",
        "    params = {\n",
        "              'n_estimators':[100,200]\n",
        "              }\n",
        "    grid = GridSearchCV(gb,params,cv=cv_sets)\n",
        "    grid = grid.fit(X,y)\n",
        "    return grid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2qPFDZhOMBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "4e279960-db02-4594-fed4-1cb5fa8be671"
      },
      "source": [
        "fit_model(X_train,y_train)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                           n_iter_no_change=None, presort='auto',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoiNo5znOMBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.model_selection import learning_curve, GridSearchCV\n",
        "def fit_model(X,y):\n",
        "    cv_sets = ShuffleSplit(n_splits=10,test_size=0.20,random_state=1234)\n",
        "    lda = LinearDiscriminantAnalysis()\n",
        "    params = {'solver' :('svd','eigen','lsqr')\n",
        "              }\n",
        "    grid = GridSearchCV(lda,params,cv=cv_sets)\n",
        "    grid = grid.fit(X,y)\n",
        "    return grid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrwuoNYvOMBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "1d5875c7-d0df-49f2-f718-46b1fe600571"
      },
      "source": [
        "fit_model(X_train,y_train)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
              "                           solver='eigen', store_covariance=False, tol=0.0001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyjKHPzsOMBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nJhFZdAOMBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize all the classifiers with the best parameters from grid search\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "            metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
        "            weights='uniform'),\n",
        "    \n",
        "    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
        "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
        "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
        "  tol=0.001, verbose=False),\n",
        "    \n",
        "    NuSVC(cache_size=200, class_weight=None, coef0=0.0,\n",
        "   decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
        "   max_iter=-1, nu=0.05, probability=True, random_state=None,\n",
        "   shrinking=True, tol=0.001, verbose=False),\n",
        "    \n",
        "    DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
        "            max_features=None, max_leaf_nodes=None,\n",
        "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "            min_samples_leaf=1, min_samples_split=2,\n",
        "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
        "            splitter='best'),\n",
        "    \n",
        "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
        "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
        "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "            min_samples_leaf=1, min_samples_split=2,\n",
        "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
        "            oob_score=False, random_state=None, verbose=0,\n",
        "            warm_start=False),\n",
        "    \n",
        "    AdaBoostClassifier(algorithm='SAMME', base_estimator=None, learning_rate=1.0,\n",
        "          n_estimators=200, random_state=None),\n",
        "    \n",
        "    GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
        "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
        "              max_features=None, max_leaf_nodes=None,\n",
        "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "              min_samples_leaf=1, min_samples_split=2,\n",
        "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
        "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
        "              warm_start=False),\n",
        "    \n",
        "    GaussianNB(),\n",
        "    \n",
        "    LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
        "              solver='svd', store_covariance=False, tol=0.0001),\n",
        "    \n",
        "    QuadraticDiscriminantAnalysis()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pih9C4puOMBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logging for Visual Comparison\n",
        "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
        "log = pd.DataFrame(columns=log_cols)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1nSWoFiOMBc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "a327edaa-9088-4a66-989f-ca21c099ad0e"
      },
      "source": [
        "for clf in classifiers:\n",
        "    clf.fit(X_train, y_train)\n",
        "    name = clf.__class__.__name__\n",
        "    \n",
        "    print(\"=\"*30)\n",
        "    print(name)\n",
        "    \n",
        "    print('****Results****')\n",
        "    train_predictions = clf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, train_predictions)\n",
        "    print(\"Accuracy: {:.4%}\".format(acc))\n",
        "    \n",
        "    train_predictions = clf.predict_proba(X_test)\n",
        "    ll = log_loss(y_test, train_predictions)\n",
        "    print(\"Log Loss: {}\".format(ll))\n",
        "    \n",
        "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
        "    log = log.append(log_entry)\n",
        "    \n",
        "print(\"=\"*30)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==============================\n",
            "KNeighborsClassifier\n",
            "****Results****\n",
            "Accuracy: 88.8889%\n",
            "Log Loss: 1.5755075129933762\n",
            "==============================\n",
            "SVC\n",
            "****Results****\n",
            "Accuracy: 81.8182%\n",
            "Log Loss: 4.465359107637923\n",
            "==============================\n",
            "NuSVC\n",
            "****Results****\n",
            "Accuracy: 96.4646%\n",
            "Log Loss: 2.2488622624907433\n",
            "==============================\n",
            "DecisionTreeClassifier\n",
            "****Results****\n",
            "Accuracy: 27.7778%\n",
            "Log Loss: 7.713676088186076\n",
            "==============================\n",
            "RandomForestClassifier\n",
            "****Results****\n",
            "Accuracy: 98.4848%\n",
            "Log Loss: 0.7620840517393141\n",
            "==============================\n",
            "AdaBoostClassifier\n",
            "****Results****\n",
            "Accuracy: 11.1111%\n",
            "Log Loss: 4.595043250050485\n",
            "==============================\n",
            "GradientBoostingClassifier\n",
            "****Results****\n",
            "Accuracy: 59.5960%\n",
            "Log Loss: 2.451756184716006\n",
            "==============================\n",
            "GaussianNB\n",
            "****Results****\n",
            "Accuracy: 57.0707%\n",
            "Log Loss: 14.827252492813216\n",
            "==============================\n",
            "LinearDiscriminantAnalysis\n",
            "****Results****\n",
            "Accuracy: 97.9798%\n",
            "Log Loss: 0.22993448213653483\n",
            "==============================\n",
            "QuadraticDiscriminantAnalysis\n",
            "****Results****\n",
            "Accuracy: 2.0202%\n",
            "Log Loss: 33.84102333642773\n",
            "==============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAdWAI7cOMBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rGN37SKOMBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "bb79ec1d-f550-4589-ba16-96e30b7e9525"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.set_color_codes(\"muted\")\n",
        "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"r\")\n",
        "plt.xlabel('Accuracy %')\n",
        "plt.title('Classifier Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAHwCAYAAABQeHUBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yWhF1DUOMBh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "8ff7205a-400e-4246-9c6e-78a218a83698"
      },
      "source": [
        "log"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Classifier</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Log Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KNeighborsClassifier</td>\n",
              "      <td>88.888889</td>\n",
              "      <td>1.575508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SVC</td>\n",
              "      <td>81.818182</td>\n",
              "      <td>4.465359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NuSVC</td>\n",
              "      <td>96.464646</td>\n",
              "      <td>2.248862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>DecisionTreeClassifier</td>\n",
              "      <td>27.777778</td>\n",
              "      <td>7.713676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>98.484848</td>\n",
              "      <td>0.762084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AdaBoostClassifier</td>\n",
              "      <td>11.111111</td>\n",
              "      <td>4.595043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>59.595960</td>\n",
              "      <td>2.451756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>57.070707</td>\n",
              "      <td>14.827252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LinearDiscriminantAnalysis</td>\n",
              "      <td>97.979798</td>\n",
              "      <td>0.229934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>QuadraticDiscriminantAnalysis</td>\n",
              "      <td>2.020202</td>\n",
              "      <td>33.841023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Classifier   Accuracy   Log Loss\n",
              "0           KNeighborsClassifier  88.888889   1.575508\n",
              "0                            SVC  81.818182   4.465359\n",
              "0                          NuSVC  96.464646   2.248862\n",
              "0         DecisionTreeClassifier  27.777778   7.713676\n",
              "0         RandomForestClassifier  98.484848   0.762084\n",
              "0             AdaBoostClassifier  11.111111   4.595043\n",
              "0     GradientBoostingClassifier  59.595960   2.451756\n",
              "0                     GaussianNB  57.070707  14.827252\n",
              "0     LinearDiscriminantAnalysis  97.979798   0.229934\n",
              "0  QuadraticDiscriminantAnalysis   2.020202  33.841023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEmU2VEsOMBj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "b1b76dcd-d3e9-47fd-d00f-796a08c15cb7"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.set_color_codes(\"muted\")\n",
        "sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n",
        "plt.xlabel('Log Loss')\n",
        "plt.title('Classifier Log Loss')\n",
        "plt.show()"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAHwCAYAAABQeHUBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng2wDeAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ni3JtUPOMBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict Test Set\n",
        "favorite_clf = LinearDiscriminantAnalysis()\n",
        "favorite_clf.fit(X_train, y_train)\n",
        "test_predictions = favorite_clf.predict_proba(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmUfmmmqOMBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d2362d5-52dd-4f5e-d3e2-377e92d16d22"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(792, 192)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjg6q7jBOMBn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "38bdc354-6e80-465f-85b6-e2dc20e34700"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(198, 192)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffw9hqSjOMBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = favorite_clf.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGv7TNNXOMBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b0c0a15-705e-421f-f1d0-aca7e2f3cc37"
      },
      "source": [
        "print(classification_report(y_test,pred))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         2\n",
            "           1       1.00      1.00      1.00         2\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       1.00      1.00      1.00         2\n",
            "           4       1.00      1.00      1.00         2\n",
            "           5       1.00      1.00      1.00         2\n",
            "           6       1.00      1.00      1.00         2\n",
            "           7       1.00      1.00      1.00         2\n",
            "           8       1.00      1.00      1.00         2\n",
            "           9       1.00      1.00      1.00         2\n",
            "          10       1.00      1.00      1.00         2\n",
            "          11       1.00      1.00      1.00         2\n",
            "          12       1.00      1.00      1.00         2\n",
            "          13       1.00      1.00      1.00         2\n",
            "          14       1.00      1.00      1.00         2\n",
            "          15       1.00      1.00      1.00         2\n",
            "          16       1.00      1.00      1.00         2\n",
            "          17       1.00      1.00      1.00         2\n",
            "          18       1.00      1.00      1.00         2\n",
            "          19       1.00      1.00      1.00         2\n",
            "          20       1.00      1.00      1.00         2\n",
            "          21       1.00      1.00      1.00         2\n",
            "          22       1.00      1.00      1.00         2\n",
            "          23       1.00      1.00      1.00         2\n",
            "          24       1.00      1.00      1.00         2\n",
            "          25       1.00      1.00      1.00         2\n",
            "          26       1.00      1.00      1.00         2\n",
            "          27       1.00      1.00      1.00         2\n",
            "          28       1.00      1.00      1.00         2\n",
            "          29       1.00      1.00      1.00         2\n",
            "          30       1.00      1.00      1.00         2\n",
            "          31       0.67      1.00      0.80         2\n",
            "          32       1.00      1.00      1.00         2\n",
            "          33       1.00      1.00      1.00         2\n",
            "          34       1.00      1.00      1.00         2\n",
            "          35       1.00      1.00      1.00         2\n",
            "          36       1.00      1.00      1.00         2\n",
            "          37       0.50      0.50      0.50         2\n",
            "          38       0.50      0.50      0.50         2\n",
            "          39       1.00      1.00      1.00         2\n",
            "          40       1.00      1.00      1.00         2\n",
            "          41       1.00      1.00      1.00         2\n",
            "          42       1.00      1.00      1.00         2\n",
            "          43       1.00      1.00      1.00         2\n",
            "          44       1.00      1.00      1.00         2\n",
            "          45       1.00      1.00      1.00         2\n",
            "          46       1.00      1.00      1.00         2\n",
            "          47       1.00      1.00      1.00         2\n",
            "          48       1.00      1.00      1.00         2\n",
            "          49       1.00      1.00      1.00         2\n",
            "          50       1.00      1.00      1.00         2\n",
            "          51       1.00      1.00      1.00         2\n",
            "          52       1.00      1.00      1.00         2\n",
            "          53       1.00      1.00      1.00         2\n",
            "          54       1.00      1.00      1.00         2\n",
            "          55       1.00      1.00      1.00         2\n",
            "          56       1.00      1.00      1.00         2\n",
            "          57       1.00      1.00      1.00         2\n",
            "          58       1.00      1.00      1.00         2\n",
            "          59       1.00      1.00      1.00         2\n",
            "          60       1.00      1.00      1.00         2\n",
            "          61       1.00      1.00      1.00         2\n",
            "          62       1.00      1.00      1.00         2\n",
            "          63       1.00      1.00      1.00         2\n",
            "          64       1.00      1.00      1.00         2\n",
            "          65       1.00      1.00      1.00         2\n",
            "          66       1.00      1.00      1.00         2\n",
            "          67       1.00      1.00      1.00         2\n",
            "          68       1.00      0.50      0.67         2\n",
            "          69       1.00      1.00      1.00         2\n",
            "          70       1.00      1.00      1.00         2\n",
            "          71       1.00      1.00      1.00         2\n",
            "          72       1.00      1.00      1.00         2\n",
            "          73       1.00      0.50      0.67         2\n",
            "          74       1.00      1.00      1.00         2\n",
            "          75       1.00      1.00      1.00         2\n",
            "          76       1.00      1.00      1.00         2\n",
            "          77       1.00      1.00      1.00         2\n",
            "          78       1.00      1.00      1.00         2\n",
            "          79       1.00      1.00      1.00         2\n",
            "          80       1.00      1.00      1.00         2\n",
            "          81       0.67      1.00      0.80         2\n",
            "          82       1.00      1.00      1.00         2\n",
            "          83       1.00      1.00      1.00         2\n",
            "          84       1.00      1.00      1.00         2\n",
            "          85       1.00      1.00      1.00         2\n",
            "          86       1.00      1.00      1.00         2\n",
            "          87       1.00      1.00      1.00         2\n",
            "          88       1.00      1.00      1.00         2\n",
            "          89       1.00      1.00      1.00         2\n",
            "          90       1.00      1.00      1.00         2\n",
            "          91       1.00      1.00      1.00         2\n",
            "          92       1.00      1.00      1.00         2\n",
            "          93       1.00      1.00      1.00         2\n",
            "          94       1.00      1.00      1.00         2\n",
            "          95       1.00      1.00      1.00         2\n",
            "          96       1.00      1.00      1.00         2\n",
            "          97       1.00      1.00      1.00         2\n",
            "          98       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           0.98       198\n",
            "   macro avg       0.98      0.98      0.98       198\n",
            "weighted avg       0.98      0.98      0.98       198\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNlfL2ydOMBv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "e142b763-56e6-48ef-e3d0-9bb0a158d10d"
      },
      "source": [
        "print(confusion_matrix(y_test,pred))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 2 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}